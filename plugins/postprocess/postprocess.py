'''

This file performs operations on the html content generated by pelican

What happens here:

* Estimate read time in minutes by number of words / WPM and assign as
  attribute `readtime` to articles and pages. Equations are included,
  but if anything, they should count for even more.

* Collect all links in metadata.

* Make Images have captions and links

* Make references better by setting 'title' attribute.

* Add some commented annotations to HTML source for debugging Katex math

'''

# assumed words per minute reading speed
WPM = 230.0 # http://en.wikipedia.org/wiki/Words_per_minute

import os
import re
import math
import datetime
import sys, traceback, io
from bs4 import BeautifulSoup

from urllib import parse as urlparse

from pelican import signals

BELL = '\u0007' # terminal bell

###############################################################################

def register():
    signals.content_object_init.connect(process_soup)

def independent_traceback(func):
    '''
    Pelican suppresses tracebacks, so we print them to stdout
    '''
    def wrapped(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            with io.StringIO() as mock_stdout:
                traceback.print_exception(*sys.exc_info(), file=mock_stdout)
                sys.stderr.write(mock_stdout.getvalue())
            raise e
    return wrapped

def parse_link(content_object, l):
    parsed_url = urlparse.urlparse(l.attrs['href'])

    return {
        'id': l.attrs['id'],
        'domain': '.'.join(parsed_url.netloc.split('.')[-2:]),
        'subdomain': '.'.join(parsed_url.netloc.split('.')[:-2]),
        'path': parsed_url.path.strip('/'),
        'text': l.text,
        'href': l.attrs['href'],
        'context': get_context_string(content_object, l),
        'article_title': content_object.title,
        'article_slug': content_object.slug,
    }

def get_context_string(content_object, target):
    prev_string = ''
    for node in target.previousGenerator():

        if node.name in ['li', 'ol', 'ul']:
            break

        if prev_string.count(BELL) > prev_string.endswith(BELL):
            break

        if isinstance(node, str) and not node.endswith('.org'):
            if node == '\n':
                prev_string = BELL + prev_string

            elif node.isdigit():
                continue

            else:
                prev_string = (
                    node
                    .replace('.', BELL)
                    .replace(':', BELL)
                    .replace(';', BELL)
                ) + prev_string

    # strip anything between '<' '>' pairs
    prev_string = re.sub('</?[^<>]*>', '', prev_string)

    if prev_string.endswith(BELL):
        prev_string = prev_string[:-1]

    # get last sentence and remove extra spaces
    return ' '.join(prev_string.split(BELL)[-1].split())


@independent_traceback
def process_soup(content_object):
    if content_object._content is None:
        return

    if hasattr(content_object, 'last_updated'):
        content_object.last_updated = datetime.datetime.strptime(
            content_object.last_updated, '%Y-%m-%d'
        )
    else:
        content_object.last_updated = content_object.date

    # distinguish pages from projects
    setattr(content_object, 'standalone', (
        'projects' in content_object.source_path.split(os.sep)
    ))

    soup = BeautifulSoup(content_object._content, 'html.parser')

    # estimate readtime
    content_object.readtime = {
        'minutes': math.ceil(len(soup.get_text().split()) / WPM)
    }

    a_id = 0
    # give links unique ids
    for l in soup.find_all('a'):
        if not 'id' in l.attrs:
            l.attrs['id'] = "generated-link-id" + str(a_id)
            a_id += 1

    # collect links
    all_links = [l for l in soup.find_all('a') if ('href' in l.attrs)]
    content_object.links = [
        parse_link(content_object, l) for l in all_links if any([
            l.attrs['href'].startswith('http'),
            l.attrs['href'].startswith('www')
        ])
    ]

    # make images better by allowing full screen interaction and caption
    img_id = 0
    for img in soup.find_all('img'):
        img_src = img.attrs['src']
        img_alt = img.attrs['alt']
        if not 'id' in img.attrs:
            img.attrs['id'] = "generated-img-id" + str(img_id)
            img_id += 0

        img.replaceWith(
            BeautifulSoup(
                img.decode() + '\n<p class="caption">' + img_alt + '</p>',
                'html.parser'
            )
        )

    # inlinesvg
    isvg_id = 0
    for isvg in soup.find_all('inlinesvg'):
        img_src = isvg.attrs['src']

        if not 'id' in isvg.attrs:
            isvg.attrs['id'] = "generated-isvg-id" + str(isvg_id)
            isvg_id += 1

        with open(img_src) as f:
            content = f.read()

        s = BeautifulSoup(content, 'html.parser')

        r = s.find('svg')

        r.attrs['id'] = isvg.attrs['id']

        if 'width' in r.attrs:
            del(r.attrs['width'])
        if 'height' in r.attrs:
            del(r.attrs['height'])

        if 'class' in isvg.attrs:
            r.attrs['class'] = isvg.attrs['class']
        else:
            r.attrs['class'] = 'inlinesvg'

        isvg.replaceWith(s)

    # preview for Wikipedia links
    for a in soup.find_all('a'):
        if 'href' in a.attrs:
            if a.attrs['href'].startswith(
                    'https://en.wikipedia.org/wiki/'
            ):

                targetname = a.attrs['href'][len('https://en.wikipedia.org/wiki/'):]

                a.attrs['class'] = 'wikipedia_link'
                a.attrs['endpoint'] = \
                    'https://en.wikipedia.org/api/rest_v1/page/summary/{}'.format(
                        targetname
                    )

    # extract context of references to display as title attribute to backlinks
    for backref in soup.find_all(attrs={'class': 'footnote-backref'}):
        target = soup.find(attrs={'id': backref.attrs['href'].strip('#')})

        if not target:
            print(
                os.path.basename(content_object.source_path),
                'has an unused reference:',
                '"{}"'.format(backref.attrs['href'].strip('#fnref-'))
            )
            backref.extract()
            continue

        backref.attrs['title'] = get_context_string(content_object, target)

    def myfunc(x):
        return x.strip()

    # prettify the math items
    for math_item in soup.find_all(attrs={'class': 'math'}):

        lvl = 0

        # white space must be respected, and getting around it with
        # comments for nice indentation looks ugly in the debugger
        transformed = math_item.decode()

        tex = math_item.find('annotation').get_text()

        math_item.replaceWith(BeautifulSoup(
            '\n<!--' + tex + '-->\n' + transformed + '\n',
            'html.parser')
        )

    content_object._content = soup.decode()
